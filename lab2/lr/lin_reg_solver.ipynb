{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6686bba1-29e3-43f0-8f2a-1f53fe3f9082",
   "metadata": {},
   "source": [
    "# Part 1: Using Gradient Descent to Solve Linear Regression\n",
    "\n",
    "In this part of the lab, you will be requested to solve the linear regression problem using gradient descent. This means you will not use the closed form solution of linear regression to find the model's parameters, but gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862e89f-0bf8-486e-a4b8-e5cb120445ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2bc74d-aacf-4414-af1d-7f42a365a904",
   "metadata": {},
   "source": [
    "For this exercise we will use some dummy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d0420-f883-432a-998d-1a42b0f1d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use some dummy data\n",
    "data = pd.read_csv('data/data.csv')\n",
    "X = data.iloc[:, 0]\n",
    "Y = data.iloc[:, 1]\n",
    "plt.scatter(X, Y)\n",
    "plt.show()\n",
    "\n",
    "#Use a friendly format for X and y\n",
    "X = X.to_numpy().reshape(-1,1)\n",
    "y = Y.to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b566f3-81fa-4038-8fb0-fbd9bc04c946",
   "metadata": {},
   "source": [
    "The data seems to follow a linear model of order one. Hence, we will try to fit the following linear regression model:\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\mathbf{w}^{T}\\mathbf{x} + w_0\n",
    "\\end{equation}\n",
    "\n",
    "Let us recall that for linear regression, we use the sum of squares loss function:\n",
    "\\begin{equation}\n",
    "\\mathcal{L} = \\dfrac{1}{N}\\sum_{i=1}^N(y_i - \\hat{y})^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504e888-9d98-49eb-a7ad-07ea3c68c3bc",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement the gradient descent function\n",
    "You will need to complete the gradient descent function for linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f8e01-eee0-41fe-8d4d-7d369e193432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(alpha, epochs, epsilon, X, y):\n",
    "    '''\n",
    "    Performs gradient descent\n",
    "    Inputs:\n",
    "    alpha - learning rate\n",
    "    epochs - number of iterations\n",
    "    epsilon - threshold to stop iterations\n",
    "    X - Input matrix of size N x (D + 1)\n",
    "    y - Output vector of size N x 1\n",
    "\n",
    "    Outputs:\n",
    "    weights - Parameters vector of size (D + 1) x 1\n",
    "    '''\n",
    "  \n",
    "    #Initialize weights\n",
    "    weights = np.zeros((X.shape[1]+1,1))\n",
    "    \n",
    "    #Adds a one to the matrix so it copes with w_0\n",
    "    X = PolynomialFeatures(1).fit_transform(X)\n",
    "    N = len(X)\n",
    "    inv_N = float(1/N)\n",
    "    prev_gradient = 0\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        #1) Estimate y_hat\n",
    "        y_hat = np.dot(X,weights)\n",
    "        \n",
    "        #2) Estimate the gradient\n",
    "        gradient = 2 * np.dot(X.T, y_hat - y)\n",
    "        gradient_norm = np.linalg.norm(gradient)\n",
    "        \n",
    "        print ('Epoch ', e, ' weights: ', weights, 'gradient_norm: ', gradient_norm) \n",
    "        \n",
    "        #3) Check the stopping criterion\n",
    "        if np.abs(gradient_norm - prev_gradient) <= epsilon:\n",
    "            break\n",
    "            \n",
    "        #4) Update weights\n",
    "        weights = #your code here\n",
    "        \n",
    "        #5) Update previous gradient\n",
    "        prev_gradient = gradient_norm\n",
    "        \n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d86897-b462-40bd-943a-ff6b7eb1d1fa",
   "metadata": {},
   "source": [
    "Now, test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85859c14-016b-4068-b86b-021a5c13e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0001  # The learning Rate\n",
    "epochs = 1000  # The number of iterations to perform gradient descent\n",
    "weights = gradient_descent(alpha, epochs, 0.00001, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277ad49-a5d5-49a5-8a12-ba04bc14b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.dot(PolynomialFeatures(1).fit_transform(X),weights)\n",
    "plt.scatter(X, Y)\n",
    "plt.plot([min(X), max(X)], [min(y_hat), max(y_hat)], color='red') # predicted\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b286de4-2d41-442d-8110-f4d729dbbafc",
   "metadata": {},
   "source": [
    "### Question 1: Stopping criterion\n",
    "Explain what is being done at steps 3 and 5 of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e3438-0c59-4407-9c3e-10d63f1479b7",
   "metadata": {},
   "source": [
    "#Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f6b96-382b-43bf-a50a-7aeb5cf39a8f",
   "metadata": {},
   "source": [
    "### Exercise 2: Hyper-parameters\n",
    "Play a bit with the learning rate (alpha), the number of epochs and the stopping criterion. What can you say about the influence these have on your results? Make sure you document all your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44092d-7355-44b9-9410-2bef75514ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1298e-9860-413f-a92d-e39091dd7b5e",
   "metadata": {},
   "source": [
    "### Exercise 3: Comparison with closed form solution\n",
    "Use your code from lab 1 or the demo used in the course to implement the closed form solution for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124201fb-d637-4cd4-b5dd-33188bf2d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6c64e-7a0a-4284-840b-5407faded8b0",
   "metadata": {},
   "source": [
    "### Question 2: Comparison\n",
    "What are the differences between the results obtained with gradient descent and with the closed form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00cf3d5-aa82-47fa-b050-cfc7b5ce1462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c67d9c3-5ed8-41a5-b541-87faa7b4e7d3",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "Implement gradient descent using mini-batch and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324d7a5-32ac-483d-8e78-78b0dd805fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
